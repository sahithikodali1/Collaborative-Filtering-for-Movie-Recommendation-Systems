# -*- coding: utf-8 -*-
"""NLP_project_moviesdata_explicit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dTGotCyu3n9TUYls6tEOxE8KgerD8FM8
"""

#import libraries and packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
from ast import literal_eval
from datetime import datetime

!pip install tensorflow-recommenders

import tensorflow as tf
import tensorflow_recommenders as tfrs

"""### Data Processing"""

#read pdf file
def read_data(file_path):
  df = pd.read_csv(file_path)
  print(df.shape)
  print(df.head())
  return df

credits = read_data("/content/drive/MyDrive/Purdue/NLP/project/the_Movies/credits.csv")
keywords = read_data("/content/drive/MyDrive/Purdue/NLP/project/the_Movies/keywords.csv")
movies = read_data("/content/drive/MyDrive/Purdue/NLP/project/the_Movies/movies_metadata.csv")

print(movies.columns)

#drop unnecessary columns
movies = movies.drop(['belongs_to_collection', 'homepage', 'imdb_id', 'poster_path', 'status', 'title', 'video', 'runtime', 'tagline', 'original_language'], axis=1)
print(movies.shape)

#convert movieId to numeric and drop rows with this id as NaN
movies['id'] = pd.to_numeric(movies['id'], errors='coerce')

movies = movies.dropna(subset=['id'])
movies['id'] = movies['id'].astype(int)
print(movies.shape)

#merge dfs based on movieId
merged_df = pd.merge(pd.merge(movies, keywords, on='id', how='inner'), credits, on='id', how='inner')
merged_df.dropna(inplace=True)
print(merged_df.shape)
print(merged_df.columns)

#create a copy of df
df = merged_df.copy()
print(df.shape)

#convert object JSON column's respective attribute to saparate column data
def extract_text(column, obj = 'name'):
  text = literal_eval(column)

  objects = []
  for i in text:
    objects.append(i[obj])

  if len(objects) == 1:
    return "".join(objects)
  else:
    return ", ".join(objects)

df['genres'] = df['genres'].apply(extract_text)
df['production_companies'] = df['production_companies'].apply(extract_text)
df['production_countries'] = df['production_countries'].apply(extract_text)
df['spoken_languages'] = df['spoken_languages'].apply(extract_text)

df['keywords'] = df['keywords'].apply(extract_text)
df['crew'] = df['crew'].apply(extract_text)
df['actors'] = df['cast'].apply(extract_text)
df['characters'] = df['cast'].apply(extract_text, obj = 'character')

#drop cast column since actors are extracted and reset index
print(df.shape)
df.drop('cast', axis=1, inplace=True)
df = df[~df['original_title'].duplicated()]
df = df.reset_index(drop=True)
print(df.shape)
print(df.info())

#get ratings data
ratings_df = read_data("/content/drive/MyDrive/Purdue/NLP/project/the_Movies/ratings_small.csv")

#convert timestamp to datetime format and drop timestamp column
ratings_df['date'] = ratings_df['timestamp'].apply(lambda x: datetime.fromtimestamp(x))
ratings_df.drop('timestamp', axis=1, inplace=True)

print(ratings_df.head())
print(ratings_df.info())

#merge ratings with movie details df
merged_df = ratings_df.merge(df[['id', 'original_title', 'genres', 'overview']], left_on='movieId', right_on='id', how='inner')
print(merged_df.shape)

#drop additional 'id' column and reset index
merged_df.drop('id', axis=1, inplace=True)
merged_df.reset_index(drop=True, inplace=True)
print(merged_df['userId'].nunique())

#convert userId to string
merged_df['userId'] = merged_df['userId'].astype(str)
print(merged_df.info())

print(merged_df['movieId'].nunique())
print(merged_df['userId'].nunique())

#extract movieID and title as a df
movies_df = df[['id', 'original_title']]
movies_df.rename(columns={'id':'movieId'}, inplace=True)
movies_df.head()

#get movies and ratings in tensor dictionary form
ratings = tf.data.Dataset.from_tensor_slices(dict(merged_df[['userId', 'original_title', 'rating', 'genres']]))
movies = tf.data.Dataset.from_tensor_slices(dict(movies_df[['original_title']]))

ratings = ratings.map(lambda x: {"original_title": x["original_title"], "userId": x["userId"], "rating": float(x["rating"]), "genre": x["genres"]})
movies = movies.map(lambda x: x["original_title"])

print(len(movies))
print(len(ratings))

#get number of unique movie titles and user ids
movie_titles = movies.batch(1000)
user_ids = ratings.batch(1000).map(lambda x: x["userId"])

unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))
unique_user_ids = np.unique(np.concatenate(list(user_ids)))

print('Unique Movies: {}'.format(len(unique_movie_titles)))
print('Unique users: {}'.format(len(unique_user_ids)))

"""### Training"""

#define model
class DNN(tfrs.models.Model):

  def __init__(self, movies, unique_movie_titles, unique_user_ids, user_emb_dim, movie_emb_dim, rating_weights, retrieval_weights):

    super().__init__()

    self.unique_movie_titles = unique_movie_titles
    self.unique_user_ids = unique_user_ids

    #define user, movie and rating models
    self.movie_model: tf.keras.layers.Layer = tf.keras.Sequential([tf.keras.layers.StringLookup(vocabulary=unique_movie_titles, mask_token=None),
                                                                   tf.keras.layers.Embedding(len(unique_movie_titles)+1, movie_emb_dim)])

    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),
                                                                   tf.keras.layers.Embedding(len(unique_user_ids)+1, user_emb_dim)])

    self.rating_model = tf.keras.Sequential([tf.keras.layers.Dense(256, activation = 'relu'),
                                            tf.keras.layers.Dense(128, activation = 'relu'),
                                            tf.keras.layers.Dense(1),])

    #compute ranking and retrive topk
    self.get_rating: tf.keras.layers.Layer = tfrs.tasks.Ranking(loss = tf.keras.losses.MeanSquaredError(), metrics = [tf.keras.metrics.RootMeanSquaredError()])

    self.retrieve_topk: tf.keras.layers.Layer = tfrs.tasks.Retrieval(metrics = tfrs.metrics.FactorizedTopK(candidates = movies.batch(128).map(self.movie_model)))

    self.rating_weight = rating_weights
    self.retrieval_weight = retrieval_weights

  def call(self, inputs):
    #compute embeddings
    user_embeddings = self.user_model(inputs['userId'])
    movie_embeddings = self.movie_model(inputs['original_title'])
    rating_prediction = self.rating_model(tf.concat([user_embeddings, movie_embeddings], axis = 1))

    return (user_embeddings, movie_embeddings, rating_prediction)

  def compute_loss(self, inputs, training = False):
    ratings = inputs.pop("rating")
    user_embeddings, movie_embeddings, rating_prediction = self(inputs)

    #compute ranking adn retrieval loss
    rating_loss = self.get_rating(labels = ratings, predictions = rating_prediction)
    retrieval_loss = self.retrieve_topk(user_embeddings, movie_embeddings)

    total_loss = self.rating_weight * rating_loss + self.retrieval_weight * retrieval_loss

    return total_loss

#initialize model and optimizer
model = DNN(moviesar, unique_movie_titles, unique_user_ids, user_emb_dim = 64, movie_emb_dim = 64, rating_weights = 1.0, retrieval_weights = 1.0)
model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))

#split to train and test data
train_data = ratings.take(35000)
test_data = ratings.skip(35000).take(8188)

#train model
cached_train = train_data.shuffle(100000).batch(1000).cache()
cached_test = test_data.batch(1000).cache()

model.fit(cached_train, epochs=7)

"""### Evaluation"""

#evaluate the model
metrics = model.evaluate(cached_test, return_dict=True)

print(f"\nRetrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}")
print(f"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}")

#predict top-5 movies given user
def predict_movie(user, top_n = 5):
  index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)
  index.index_from_dataset(tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model))))
  _, titles = index(tf.constant([str(user)]))
  pred_movies = pd.DataFrame({'original_title': [i.decode('utf-8') for i in titles[0,:top_n].numpy()]})

  print('Top {} recommendations for user {}:\n'.format(top_n, user))

  pred_df = pred_movies.merge(merged_df[['original_title', 'genres', 'overview']], on='original_title', how='left')
  pred_df = pred_df[~pred_df['original_title'].duplicated()]
  pred_df.reset_index(drop=True, inplace=True)
  pred_df.index = np.arange(1, len(pred_df)+1)

  return pred_df
  # for i, title in enumerate(titles[0, :top_n].numpy()):
  #   title_str = title.decode("utf-8")
  #   # genre = title_genre_map.get(title_str, "N/A")  # Get genre from title_genre_map
  #   print('{}. {}'.format(i+1, title_str))

#predict the rating the user can give to a movie
def predict_rating(user, movie):
  trained_movie_embeddings, trained_user_embeddings, predicted_rating = model({"userId": np.array([str(user)]), "original_title": np.array([movie])})
  print("Predicted rating for {}: {}".format(movie, predicted_rating.numpy()[0][0]))

predict_movie(123, 5)

predict_rating(123,'Star Wars')

predict_rating(123,'Dog Day Afternoon')

predict_movie(53, 5)

predict_rating(53,'Minions')

predict_rating(53,'Secret Agent')