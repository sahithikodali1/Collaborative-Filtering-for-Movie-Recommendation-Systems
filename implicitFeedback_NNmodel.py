# -*- coding: utf-8 -*-
"""NLP_project_20Mdata_implicit_binarized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1st5TtHFp2knQ61sXnCzDuzVB21z9hLfa
"""

#import libraries and packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
from ast import literal_eval
from tqdm.notebook import tqdm

"""### Data Processing"""

#read pdf file
def read_data(file_path):
  df = pd.read_csv(file_path)
  print(df.shape)
  print(df.head())
  return df

#using 20M movies dataset
ratings_df =  read_data("/content/drive/MyDrive/Purdue/NLP/project/20M_movies/rating.csv")
movies_df =  read_data("/content/drive/MyDrive/Purdue/NLP/project/20M_movies/movie.csv")

#add genres and titles to ratings dataframe
merged_df = pd.merge(ratings_df, movies_df, on='movieId', how='left')
print(merged_df.shape)
print(merged_df.head())

#select 20% of the unique users data randomly
rand_users = np.random.choice(merged_df['userId'].unique(), size=int(len(merged_df['userId'].unique())*0.2), replace=False)
ratings_rand_df = merged_df.loc[merged_df['userId'].isin(rand_users)]

print('There are {} rows of data from {} users'.format(len(ratings_rand_df), len(rand_users)))
print(ratings_rand_df.shape)

#split data based on leave one methodology i.e keep the latest review in test data and the rest in train data based on timestamp
def get_leave1_data(df, group_by_column, what2group_column):
  df['rank_latest'] = df.groupby([group_by_column])[what2group_column].rank(method='first', ascending=False)

  train_df = df[df['rank_latest'] != 1]
  test_df = df[df['rank_latest'] == 1]

  return train_df, test_df

train_ratings, test_ratings = get_leave1_data(ratings_rand_df, 'userId', 'timestamp')

print(train_ratings.shape)
print(train_ratings.head(10))

print(test_ratings.shape)
print(test_ratings.head(10))

#drop selected columns and reset index
def drop_resetidx(df, drop_columns):
  df = df.drop(columns=drop_columns)
  df.reset_index(drop=True)
  print(df.head())
  return df

train_ratings =  drop_resetidx(train_ratings, drop_columns=['timestamp', 'rank_latest'])
test_ratings =  drop_resetidx(test_ratings, drop_columns=['timestamp', 'rank_latest'])
print(train_ratings.shape)
print(test_ratings.shape)

"""### Training"""

#install and import libraries/packages

!pip install pytorch_lightning

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl

#class to extract data by negative sampling in 1:5 ratio of positive and negative samples
class MovieDataset(Dataset):
    def __init__(self, t_ratings, all_movie_ids):
      self.users, self.movies, self.rat_binarized = self.get_data(t_ratings, all_movie_ids)

    def __len__(self):
      return len(self.users)

    def __getitem__(self, idx):
      return self.users[idx], self.movies[idx], self.rat_binarized[idx]

    def get_data(self, t_ratings, all_movie_ids):
      users, movies, rat_binarized = [], [], []
      user_mv_set = set(zip(t_ratings['userId'], t_ratings['movieId'])) #extract every user and movie pair

      num_negative_samples = 5

      for (u,m) in tqdm(user_mv_set):
        users.append(u)
        movies.append(m)
        rat_binarized.append(1) #indicate interaction

        for i in range(num_negative_samples): #perform negative sampling
          neg_mv = np.random.choice(all_movie_ids)
          while (u, neg_mv) in user_mv_set:
            neg_mv = np.random.choice(all_movie_ids)
          users.append(u)
          movies.append(neg_mv)
          rat_binarized.append(0) #indicate no interaction

      return torch.tensor(users), torch.tensor(movies), torch.tensor(rat_binarized)

#Dense neural network model
class DNN(pl.LightningModule):

  def __init__(self, num_unique_user_ids, num_unique_movie_titles, all_movie_ids, t_ratings, emb_dim):
    super().__init__()

    self.user_embedding = nn.Embedding(num_embeddings=num_unique_user_ids, embedding_dim=emb_dim)
    self.mv_embedding = nn.Embedding(num_embeddings=num_unique_movie_titles, embedding_dim=emb_dim)

    self.fc1 = nn.Linear(emb_dim*2, 64)
    self.fc2 = nn.Linear(64, 32)
    self.output = nn.Linear(32,1)

    self.t_ratings = t_ratings
    self.all_movie_ids = all_movie_ids

  def forward(self, user_input, mv_input):
    #get user and movie embeddings
    user_emb = self.user_embedding(user_input)
    mv_emb = self.mv_embedding(mv_input)

    #combine the embeddings
    combined_emb = torch.cat([user_emb, mv_emb], dim=-1)

    #pass embeddings through dense layers followed by sigmoid to classify as 1/0.
    out = nn.ReLU()(self.fc1(combined_emb))
    out = nn.ReLU()(self.fc2(out))
    pred = nn.Sigmoid()(self.output(out))

    return pred

  def training_step(self, batch):
    user_input, mv_input, rat_bin = batch
    predicted_labels = self(user_input, mv_input)

    #compute loss
    loss = nn.BCELoss()(predicted_labels, rat_bin.view(-1, 1).float())
    return loss

  def configure_optimizers(self):
    #define optimizer
    return torch.optim.Adam(self.parameters())

  def train_dataloader(self):
    #defien dataloader
    return DataLoader(MovieDataset(self.t_ratings, self.all_movie_ids), batch_size=512, num_workers=4)

#compute data required to initialize model
num_unique_user_ids = ratings_rand_df['userId'].max()+1
num_unique_movie_titles = ratings_rand_df['movieId'].max()+1
all_movie_ids = ratings_rand_df['movieId'].unique()

#initialize model
model = DNN(num_unique_user_ids, num_unique_movie_titles, all_movie_ids, train_ratings, emb_dim = 16)

#training
trainer = pl.Trainer(max_epochs=5, logger=False)
trainer.fit(model)

"""### Evaluation"""

print(len(all_movie_ids))

#create movieId and title map
movie_id_title_map = ratings_rand_df.set_index('movieId')['title'].to_dict()
print(len(movie_id_title_map))

#create movieId and genre map
movie_id_genre_map = ratings_rand_df.set_index('movieId')['genres'].to_dict()
print(len(movie_id_genre_map))

# Dict of all items that are interacted with by each user
all_user_interacted_items = ratings_rand_df.groupby('userId')['movieId'].apply(list).to_dict()

#compute hits
hits = []

#get user and movie pairs
test_user_mv_set = set(zip(test_ratings['userId'], test_ratings['movieId'], test_ratings['title'], test_ratings['genres']))
print(len(test_user_mv_set))

#compute hit ratio
for (u,m,t,g) in tqdm(test_user_mv_set):
  interacted_mvs = all_user_interacted_items[u] #get user interacted movies
  non_interacted_mvs = set(all_movie_ids) - set(interacted_mvs) #get user non-interacted movies
  selected_non_interacted_mvs = list(np.random.choice(list(non_interacted_mvs), 99))
  test_mvs_all = selected_non_interacted_mvs + [m] #all non interacted movies list including the test data interacted one

  predicted_rat_bin = np.squeeze(model(torch.tensor([u]*100), torch.tensor(test_mvs_all)).detach().numpy()) #predict movies

  top10_indices = np.argsort(predicted_rat_bin)[::-1][:10]  # Get indices of top 10 movies
  top10_movies = [test_mvs_all[i] for i in top10_indices]  # Get top 10 movies

  # Print top 10 movies and their corresponding predicted movies
  print("User:", u, "Interacted MovieId (as per test data):", m, "Title:", t, "Genre:", g, "\n")
  print("Predicted top 10 Movies:")
  for i, idx in enumerate(top10_indices):
    movie_id = test_mvs_all[idx]
    title = movie_id_title_map.get(movie_id, "N/A")  # Get title from movie_id_title_map
    genre = movie_id_genre_map.get(movie_id, "N/A")  # Get genre from movie_id_genre_map
    print("MovieId:", movie_id, "Title:", title, "Genre:", genre, )
  print("\n", "--------------------------------------------------------------", "\n")

  #check for hit i.e if interacted movie is in predicted movies
  if m in top10_movies:
      hits.append(1)
  else:
      hits.append(0)

print("The Hit Ratio @ 10 is {:.2f}".format(np.average(hits)))